{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RADI608: Data Mining and Machine Learning**\n",
    "\n",
    "### Assignment: K-Nearest Neighbors \n",
    "**Romen Samuel Rodis Wabina** <br>\n",
    "Student, PhD Data Science in Healthcare and Clinical Informatics <br>\n",
    "Clinical Epidemiology and Biostatistics, Faculty of Medicine (Ramathibodi Hospital) <br>\n",
    "Mahidol University"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets\n",
    "from time import time\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, make_scorer, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "#### Perform K-Nearest Neighbors to predict patient have a cancer using <code>weights = 'distance'</code>.\n",
    "\n",
    "**SOLUTION** <br>\n",
    "The class distribution is imbalanced with 22 patients having cancer while 40 patients having no cancer. Hence, it is important to perform resampling techniques (i.e., undersampling, oversampling, SMOTE). Here, we performed oversampling using Synthetic Minority Oversampling Technique (SMOTE) as presented below as <code>smote = SMOTE()</code>. We also verified the dataset in terms of their data types (i.e., <code>float</code> for $\\mathbf{X}$ while <code>int</code> for $\\mathbf{y}$) to ensure proper data modeling in KNN. No missing values were detected in the dataset. \n",
    "\n",
    "In addition, we normalize the given dataset through standardization by removing the mean and scaling to unit variance since it is possible that they are measured in different units. We also split the data into 80:20 ratio between training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_colon = pd.read_csv('../data/colon.csv')\n",
    "XX = df_colon.drop('Class', axis = 1)\n",
    "yy = df_colon['Class']\n",
    "\n",
    "X, y = XX.to_numpy(), yy.to_numpy()\n",
    "y = y.flatten()\n",
    "\n",
    "smote = SMOTE()\n",
    "X, y = smote.fit_resample(X, y)\n",
    "\n",
    "random.seed(413)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "random.seed(413)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used <code>weights = 'distance'</code> for this KNN model, that initializes the weight assigned to points in the neighbourhood. Since we used <code>distance</code>, the closer neighbours of a query point will have a greater influence than neighbours which are further away. In the code below, we initialized the <code>n_neighbors</code> as a range between $[2, 12)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(y, prediction, set = 'Training Set'):\n",
    "     print(f'======================= {set} =======================')\n",
    "     print(confusion_matrix(y, prediction))\n",
    "     print(classification_report(y, prediction, target_names = ['No Cancer', 'Cancer']))\n",
    "\n",
    "def main(X_train, X_test, y_train, y_test, model, param_grid = {'n_neighbors': [1, 2, 4, 6, 8, 10, 12]}, display_train = True):\n",
    "     start = time()\n",
    "     \n",
    "     cv = StratifiedShuffleSplit(n_splits = 10, random_state = 42)\n",
    "     \n",
    "     random.seed(413)\n",
    "     grid = GridSearchCV(model, param_grid = param_grid, cv = cv, refit = 'recall')\n",
    "     grid.fit(X_train, y_train) \n",
    "\n",
    "     print(f'The best parameters are {grid.best_params_} with\" + f\" a score of {grid.best_score_:.2f}')\n",
    "     \n",
    "     predictions = grid.predict(X_train)\n",
    "     yhat = grid.predict(X_test)\n",
    "     print(f\"Fit and predict time: {np.round(time() - start, 4)} seconds\")\n",
    "     if display_train:\n",
    "          display_results(y_train, predictions, set = 'Training Set')\n",
    "     display_results(y_test, yhat, set = 'Testing Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'n_neighbors': 12} with\" + f\" a score of 0.91\n",
      "Fit and predict time: 10.198 seconds\n",
      "======================= Testing Set =======================\n",
      "[[7 1]\n",
      " [0 8]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   No Cancer       1.00      0.88      0.93         8\n",
      "      Cancer       0.89      1.00      0.94         8\n",
      "\n",
      "    accuracy                           0.94        16\n",
      "   macro avg       0.94      0.94      0.94        16\n",
      "weighted avg       0.94      0.94      0.94        16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_distance = KNeighborsClassifier(weights = 'distance')\n",
    "main(X_train, X_test, y_train, y_test, model_distance, display_train = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameter for <code>KNN('distance')</code> is <code>k = 12</code>. This implies that we consider twelve nearest neighbors to get its *majority* vote and determine the classification of a given sample based on the vote. Hence, a patient will be assigned to the same class (i.e., Cancer or No Cancer) as to its twelve nearest neighbors due to similarity of their features (i.e., independent variables). The <code>KNN('distance')</code> took 10.20 seconds to fit the training set and predict the testing data. This may imply that <code>KNN</code>, in general, is costly to calculate distance on large datasets.\n",
    "\n",
    "The <code>SVM(weights = 'distance')</code>, however, still produced excellent performance metrics. Results have shown that there is only one misclassification among the two classes in the given dataset - that is, one false positive prediction, indicating that a patient has been classified by the model with cancer but the ground truth indicates otherwise. While there exists a single misclassification, the results still generated good <code>recall</code> for both classes: 88% for No Cancer while 100% for Cancer classes. It is important that we use <code>recall</code> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "#### Perform KNN to predict patient have a cancer using <code>weights = 'uniform'</code>\n",
    "\n",
    "We used <code>weights = 'uniform'</code> for this KNN model, that initializes the weight assigned to points in the neighbourhood. Since we used <code>uniform</code>, this means that all neighbors get an equally weighted *vote* about an observation's class. In the code below, we initialized the <code>n_neighbors</code> as a range between $[2, 12)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'n_neighbors': 8} with\" + f\" a score of 0.90\n",
      "Fit and predict time: 9.7412 seconds\n",
      "======================= Testing Set =======================\n",
      "[[7 1]\n",
      " [1 7]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   No Cancer       0.88      0.88      0.88         8\n",
      "      Cancer       0.88      0.88      0.88         8\n",
      "\n",
      "    accuracy                           0.88        16\n",
      "   macro avg       0.88      0.88      0.88        16\n",
      "weighted avg       0.88      0.88      0.88        16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_distance = KNeighborsClassifier(weights = 'uniform')\n",
    "main(X_train, X_test, y_train, y_test, model_distance, display_train = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameter for <code>KNN('uniform')</code> is <code>k = 8</code>. This implies that we consider eight nearest neighbors and get its *majority* vote to determine the classification of a given sample. Hence, a patient will be assigned to the same class (i.e., Cancer or No Cancer) as to its eight nearest neighbors due to similarity of their features (i.e., independent variables). The <code>KNN('uniform')</code> fitted and predicted the datasets at almost 10 seconds, which is approximately the same with the <code>KNN('uniform')</code>.\n",
    "\n",
    "While <code>KNN('uniform')</code> produced good performance metrics, its scores are still comparatively lower than in <code>KNN('distance')</code>, particularly the <code>recall</code> metrics where both 'cancer' and 'no cancer' class have 88% recall. Results have shown that <code>KNN('uniform')</code> produced one false positive and one false negative predictions, thus, having misclassifications to its results. We already know what false positive implies. Meanwhile, the single false negative result indicates that the model misclassified one patient as healthy but the ground truth says the patient has colon cancer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "- The <code>weights = 'uniform'</code> generated more **misclassifications** than in <code>weights = 'distance'</code>. One possible reason for <code>uniform</code>'s low performance metrics is mainly based on how every neighborhood points get an equally-weighted vote (i.e., thus, *uniform*) about the sample's class regardless of its distance to the sample. Far neighbors, despite having different classes, can still contribute to the majority vote of sample's class since every neighbor, regardless of its distance, has an equal vote. In other words, <code>weights = 'uniform'</code> ensures that even if some of the nearest neighbors are a bit further away, they still count as much towards the prediction, regardless if their classes are different to nearer neighbors. Therefore, <code>weights = 'uniform'</code> is most likely prone to misclassifications.\n",
    "\n",
    "- From our point above, it is expected that <code>weights = 'distance'</code> can **reduce** misclassifications since it prioritizes nearer neighbors. The nearer neighbors are most likely contain similar classes with the sample - having classification results that are more accurate. Furthermore, <code>distance</code> reduces bias by down-weighting data points that are less similar, but this increases variance because the prediction relies more on individual data points from the training sample.\n",
    "\n",
    "- Since <code>weights = 'uniform'</code> takes into account far neighbors, this strategy produces lower dispersions among its predictions since each of the nearest neighbors has the same contribution. This implies that uniform-based weighting strategy in KNN provides lower variance and higher bias than <code>weights = 'distance'</code>. Low variance and high bias highly correspond to underfitting, which may likely produce misclassifications. \n",
    "\n",
    "- This also implies that uniform-based weighting strategy provides higher variance and lower bias than <code>weights = 'distance'</code>. Hence, we may expect that <code>weights = 'distance'</code> would tend to overfit since it can overly prioritize the closest neighbor and disregard the other nearest neighbors if they are bit a further away.    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "c6a2dc670f3436433c0efae6fb324965c1072d8aef0b90287abce79ee9328779"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
