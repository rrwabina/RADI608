{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What is Reinforcement Learning?\n",
    "\n",
    "Reinforcement Learning (RL) is the science of decision making. It is a type of machine learning technique that enables an **agent** to learn in an **interactive environment** by *trial and error* using feedback from its own actions and experiences in order to maximize a reward.\n",
    "\n",
    "##### 1.1 Differences of RL to Supervised and Unsupervised Learning\n",
    "In supervised and reinforcement learning, both methods use mapping between input and output. However, unlike supervised learning where the feedback provided to the agent is correct set of actions for performing a task, **reinforcement learning uses rewards and punishments as signals for positive and negative behavior.** As compared to unsupervised learning, reinforcement learning is different in terms of goals. While the goal in unsupervised learning is to find similarities and differences between data points, in the case of reinforcement learning the goal is to find a suitable action model that would maximize the total cumulative reward of the agent. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src = \"../figures/types.PNG\" width = \"500\"/> <br>\n",
    "Source: https://towardsdatascience.com/reinforcement-learning-101-e24b50e1d292\n",
    "</center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does Reinforcement Learning learn?** The agent receives positive or negative rewards based on its actions and uses this feedback to improve its behavior. The figure below illustrates the action-reward feedback loop of a generic RL model.\n",
    "\n",
    "<center>\n",
    "<img src = \"../figures/RLmodel.PNG\" width = \"500\"/> <br>\n",
    "Source: https://towardsdatascience.com/reinforcement-learning-101-e24b50e1d292\n",
    "</center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Examples of reinforcement learning\n",
    "\n",
    "One example of reinforcement learning is a robot learning to walk. The robot receives a positive reward for each step it takes without falling, and a negative reward for each time it falls. Over time, the robot learns to take better steps in order to maximize its reward.\n",
    "\n",
    "Another example is a self-driving car learning to navigate city streets. The car receives a positive reward for reaching its destination safely and efficiently, and a negative reward for crashing or breaking traffic laws. By learning from its mistakes, the car can improve its driving behavior over time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. How does it work?\n",
    "In reinforcement learning, an agent learns through trial and error. It takes actions in an environment and receives rewards or punishments based on those actions. The goal is for the agent to learn the best actions to take in a given state in order to maximize the reward.\n",
    "\n",
    "To do this, the agent needs to estimate the value of each action it can take in a given state. The value of an action is the expected reward the agent will receive if it takes that action. The agent can then choose the action with the highest value, or it can use a technique called \"exploration\" to try out different actions and see which ones lead to the most reward."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Key terms\n",
    "Some key terms that describe the basic elements of an RL problem are:\n",
    "- Agent: The entity that is learning to interact with the environment.\n",
    "- Environment: The world in which the agent takes actions and receives rewards.\n",
    "- State: The current situation or condition of the agent and the environment.\n",
    "- Action: A choice that the agent can make in a given state.\n",
    "- Reward: A positive or negative value given to the agent based on its actions.\n",
    "- Value: The expected reward for taking a given action in a given state.\n",
    "- Policy: The strategy that the agent uses to choose actions based on the values of those actions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. How does RL work?\n",
    "\n",
    "An RL problem can be best explained through games. Let’s take the game of PacMan where the goal of the agent(PacMan) is to eat the food in the grid while avoiding the ghosts on its way. In this case, the grid world is the interactive environment for the agent where it acts. Agent receives a reward for eating food and punishment if it gets killed by the ghost (loses the game). The states are the location of the agent in the grid world and the total cumulative reward is the agent winning the game."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src = \"../figures/pacman.GIF\" width = \"500\"/> <br>\n",
    "Source: https://towardsdatascience.com/reinforcement-learning-101-e24b50e1d292\n",
    "</center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Markov Decision Processes (MDPs)\n",
    "\n",
    "Markov Decision Processes(MDPs) are mathematical frameworks to describe an environment in RL and almost all RL problems can be formulated using MDPs. An MDP consists of a set of finite environment states $S$, a set of possible actions $A(s)$ in each state, a real valued reward function $R(s)$ and a transition model $P(s’,s|a)$. However, real world environments are more likely to lack any prior knowledge of environment dynamics. Model-free RL methods come handy in such cases.\n",
    "\n",
    "An MDP is used to represent the environment in which an agent is learning to interact. The states represent the different situations or conditions that the agent can be in, the actions represent the choices that the agent can make in each state, and the reward function specifies the rewards or punishments that the agent receives for taking certain actions in certain states.\n",
    "\n",
    "To solve an MDP, the agent must learn a policy that defines the best action to take in each state. This policy can be represented as a mapping from states to actions, or it can be represented as a function that takes a state as input and outputs the action to take in that state.\n",
    "\n",
    "The goal of the agent is to learn a policy that will maximize the expected reward over time. This is done through trial and error, where the agent takes actions in the environment and receives rewards or punishments based on those actions. The agent can use this feedback to update its policy and improve its behavior."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Q-Learning \n",
    "It is used to learn the optimal action-selection policy for a given environment. In Q-learning, an agent tries to learn the best action to take in each state of the environment by using a **Q-table**. \n",
    "\n",
    "The Q-table is a grid that has one row for each state and one column for each action. The cell at the intersection of a row and column represents the estimated reward for taking a specific action in a specific state.\n",
    "\n",
    "For example, consider a robot that is trying to learn to navigate a maze. The robot can move left, right, up, or down at each step. The states of the environment could be the different locations within the maze, and the actions could be the different directions the robot can move. The Q-table would have a row for each location in the maze and a column for each direction the robot can move. The cell at the intersection of a row and column would represent the estimated reward for moving in a specific direction from a specific location.\n",
    "\n",
    "To learn the optimal policy, the Q-learning algorithm follows a loop that consists of the following steps:\n",
    "\n",
    "1. The agent selects an action based on the current state and its Q-table. It can either choose the action with the highest estimated reward, or it can use a technique called \"exploration\" to try out different actions and see which ones lead to the most reward.\n",
    "2. The agent takes the selected action and observes the resulting state and reward.\n",
    "3. The agent updates the Q-table to reflect the observed reward and the estimated rewards of the actions it can take in the resulting state.\n",
    "4. The agent repeats the process, selecting actions based on the updated Q-table.\n",
    "\n",
    "Over time, the Q-table will converge to the optimal action-selection policy, which is the policy that will lead to the most reward in the long run.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3608206ce1eb198bd23abae205dd191f991de1c92dbe872a18ef9e948d8a869d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
