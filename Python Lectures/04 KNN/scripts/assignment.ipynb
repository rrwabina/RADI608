{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RADI608: Data Mining and Machine Learning**\n",
    "\n",
    "### Assignment: K-Nearest Neighbors \n",
    "**Romen Samuel Rodis Wabina** <br>\n",
    "Student, PhD Data Science in Healthcare and Clinical Informatics <br>\n",
    "Clinical Epidemiology and Biostatistics, Faculty of Medicine (Ramathibodi Hospital) <br>\n",
    "Mahidol University"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, make_scorer, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "#### Perform K-Nearest Neighbors to predict patient have a cancer using <code>weights = 'distance'</code>.\n",
    "\n",
    "**SOLUTION** <br>\n",
    "The class distribution is imbalanced with 22 patients having cancer while 40 patients having no cancer. Hence, it is important to perform resampling techniques (i.e., undersampling, oversampling, SMOTE). Here, we performed oversampling using Synthetic Minority Oversampling Technique (SMOTE) as presented below as <code>smote = SMOTE()</code>. We also verified the dataset in terms of their data types (i.e., <code>float</code> for $\\mathbf{X}$ while <code>int</code> for $\\mathbf{y}$) to ensure proper data modeling in KNN. No missing values were detected in the dataset. In addition, we normalize the given dataset through standardization by removing the mean and scaling to unit variance. We also split the data into 80:20 ratio between training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_colon = pd.read_csv('../data/colon.csv')\n",
    "XX = df_colon.drop('Class', axis = 1)\n",
    "yy = df_colon['Class']\n",
    "\n",
    "X, y = XX.to_numpy(), yy.to_numpy()\n",
    "y = y.flatten()\n",
    "\n",
    "smote = SMOTE()\n",
    "X, y = smote.fit_resample(X, y)\n",
    "\n",
    "random.seed(413)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used <code>weights = 'distance'</code> for this KNN model, that initializes the weight assigned to points in the neighbourhood. Since we used <code>distance</code>, the closer neighbours of a query point will have a greater influence than neighbours which are further away. In the code below, we initialized the <code>n_neighbors</code> as a range between $[2, 12)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(X_train, X_test, y_train, y_test, \n",
    "         model, param_grid = {'n_neighbors': np.arange(1, 13, 1)}):\n",
    "\n",
    "    random.seed(413)\n",
    "    cv = StratifiedShuffleSplit(n_splits = 10, random_state = 42)\n",
    "    grid = GridSearchCV(model, param_grid = param_grid, cv = cv, refit = 'f1_micro')\n",
    "    grid.fit(X_train, y_train) \n",
    "\n",
    "    print(f\"The best parameters are {grid.best_params_} with\" + f\" a score of {grid.best_score_:.2f}\")\n",
    "    \n",
    "    yhat = grid.predict(X_test)\n",
    "\n",
    "    print('======================= Confusion Matrix =======================')\n",
    "    print(confusion_matrix(y_test, yhat))\n",
    "    \n",
    "    print('==================== Classification Report =====================')\n",
    "    print(classification_report(y_test, yhat, target_names = ['Cancer', 'No Cancer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'n_neighbors': 3} with a score of 0.86\n",
      "======================= Confusion Matrix =======================\n",
      "[[7 1]\n",
      " [1 7]]\n",
      "==================== Classification Report =====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Cancer       0.88      0.88      0.88         8\n",
      "   No Cancer       0.88      0.88      0.88         8\n",
      "\n",
      "    accuracy                           0.88        16\n",
      "   macro avg       0.88      0.88      0.88        16\n",
      "weighted avg       0.88      0.88      0.88        16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_distance = KNeighborsClassifier(weights = 'distance')\n",
    "main(X_train, X_test, y_train, y_test, model_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "#### Perform KNN to predict patient have a cancer using <code>weights = 'uniform'</code>\n",
    "\n",
    "We used <code>weights = 'uniform'</code> for this KNN model, that initializes the weight assigned to points in the neighbourhood. Since we used <code>distance</code>, the closer neighbours of a query point will have a greater influence than neighbours which are further away. In the code below, we initialized the <code>n_neighbors</code> as a range between $[2, 12)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'n_neighbors': 10} with a score of 0.90\n",
      "======================= Confusion Matrix =======================\n",
      "[[6 2]\n",
      " [0 8]]\n",
      "==================== Classification Report =====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Cancer       1.00      0.75      0.86         8\n",
      "   No Cancer       0.80      1.00      0.89         8\n",
      "\n",
      "    accuracy                           0.88        16\n",
      "   macro avg       0.90      0.88      0.87        16\n",
      "weighted avg       0.90      0.88      0.87        16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_distance = KNeighborsClassifier(weights = 'uniform')\n",
    "main(X_train, X_test, y_train, y_test, model_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "We used <code>weights = 'uniform'</code> for this KNN model, that initializes the weight assigned to points in the neighbourhood. Since we used <code>distance</code>, the closer neighbours of a query point will have a greater influence than neighbours which are further away. In the code below, we initialized the <code>n_neighbors</code> as a range between $[2, 12)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6a2dc670f3436433c0efae6fb324965c1072d8aef0b90287abce79ee9328779"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
