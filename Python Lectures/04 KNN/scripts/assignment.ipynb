{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RADI608: Data Mining and Machine Learning**\n",
    "\n",
    "### Assignment: K-Nearest Neighbors \n",
    "**Romen Samuel Rodis Wabina** <br>\n",
    "Student, PhD Data Science in Healthcare and Clinical Informatics <br>\n",
    "Clinical Epidemiology and Biostatistics, Faculty of Medicine (Ramathibodi Hospital) <br>\n",
    "Mahidol University"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, make_scorer, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "#### Perform K-Nearest Neighbors to predict patient have a cancer using <code>weights = 'distance'</code>.\n",
    "\n",
    "**SOLUTION** <br>\n",
    "The class distribution is imbalanced with 22 patients having cancer while 40 patients having no cancer. Hence, it is important to perform resampling techniques (i.e., undersampling, oversampling, SMOTE). Here, we performed oversampling using Synthetic Minority Oversampling Technique (SMOTE) as presented below as <code>smote = SMOTE()</code>. We also verified the dataset in terms of their data types (i.e., <code>float</code> for $\\mathbf{X}$ while <code>int</code> for $\\mathbf{y}$) to ensure proper data modeling in KNN. No missing values were detected in the dataset. In addition, we normalize the given dataset through standardization by removing the mean and scaling to unit variance. We also split the data into 80:20 ratio between training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_colon = pd.read_csv('../data/colon.csv')\n",
    "XX = df_colon.drop('Class', axis = 1)\n",
    "yy = df_colon['Class']\n",
    "\n",
    "X, y = XX.to_numpy(), yy.to_numpy()\n",
    "y = y.flatten()\n",
    "\n",
    "smote = SMOTE()\n",
    "X, y = smote.fit_resample(X, y)\n",
    "\n",
    "random.seed(413)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used <code>weights = 'distance'</code> for this KNN model, that initializes the weight assigned to points in the neighbourhood. Since we used <code>distance</code>, the closer neighbours of a query point will have a greater influence than neighbours which are further away. In the code below, we initialized the <code>n_neighbors</code> as a range between $[2, 12)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def display_results(y, prediction, set = 'Training Set'):\n",
    "     print(f'======================= {set} =======================')\n",
    "     print(confusion_matrix(y, prediction))\n",
    "     print(classification_report(y, prediction, target_names = ['No Cancer', 'Cancer']))\n",
    "\n",
    "def main(X_train, X_test, y_train, y_test, model, param_grid = {'n_neighbors': [1, 2, 4, 6, 8, 10, 12]}, display_train = True):\n",
    "     \n",
    "     random.seed(413)\n",
    "     cv = StratifiedShuffleSplit(n_splits = 10, random_state = 42)\n",
    "     grid = GridSearchCV(model, param_grid = param_grid, cv = cv, refit = 'f1_micro')\n",
    "     grid.fit(X_train, y_train) \n",
    "\n",
    "     print(f'The best parameters are {grid.best_params_} with\" + f\" a score of {grid.best_score_:.2f}')\n",
    "     \n",
    "     predictions = grid.predict(X_train)\n",
    "     yhat = grid.predict(X_test)\n",
    "     \n",
    "     if display_train:\n",
    "          display_results(y_train, predictions, set = 'Training Set')\n",
    "     display_results(y_test, yhat, set = 'Testing Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'n_neighbors': 8} with\" + f\" a score of 0.91\n",
      "======================= Testing Set =======================\n",
      "[[7 1]\n",
      " [0 8]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   No Cancer       1.00      0.88      0.93         8\n",
      "      Cancer       0.89      1.00      0.94         8\n",
      "\n",
      "    accuracy                           0.94        16\n",
      "   macro avg       0.94      0.94      0.94        16\n",
      "weighted avg       0.94      0.94      0.94        16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_distance = KNeighborsClassifier(weights = 'distance')\n",
    "main(X_train, X_test, y_train, y_test, model_distance, display_train = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameter for <code>KNN('distance')</code> is <code>k = 8</code>. This implies that we consider eight nearest neighbors to determine the classification of a given sample. Hence, a patient will be assigned to the same class (i.e., Cancer or No Cancer) as to its eight nearest neighbors due to similarity of their features (i.e., independent variables).\n",
    "\n",
    "The <code>SVM(weights = 'distance')</code> produced excellent performance metrics. Results have shown that there is only one misclassification among the two classes in the given dataset - that is, one false positive prediction. While there exists a single misclassification, the results still generated excellent <code>F1-score</code> for both classes: 93% for No Cancer while 94% for Cancer classes. It is important that we will use F1-score as the primary performance metrics since we utilized a high-dimensional dataset (i.e., number of features is larger than samples). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "#### Perform KNN to predict patient have a cancer using <code>weights = 'uniform'</code>\n",
    "\n",
    "We used <code>weights = 'uniform'</code> for this KNN model, that initializes the weight assigned to points in the neighbourhood. Since we used <code>uniform</code>, this means that all neighbors get an equally weighted *vote* about an observation's class. In the code below, we initialized the <code>n_neighbors</code> as a range between $[2, 12)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'n_neighbors': 6} with\" + f\" a score of 0.87\n",
      "======================= Testing Set =======================\n",
      "[[7 1]\n",
      " [1 7]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   No Cancer       0.88      0.88      0.88         8\n",
      "      Cancer       0.88      0.88      0.88         8\n",
      "\n",
      "    accuracy                           0.88        16\n",
      "   macro avg       0.88      0.88      0.88        16\n",
      "weighted avg       0.88      0.88      0.88        16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_distance = KNeighborsClassifier(weights = 'uniform')\n",
    "main(X_train, X_test, y_train, y_test, model_distance, display_train = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameter for <code>KNN('uniform')</code> is <code>k = 6</code>. This implies that we consider six nearest neighbors to determine the classification of a given sample. Hence, a patient will be assigned to the same class (i.e., Cancer or No Cancer) as to its six nearest neighbors due to similarity of their features (i.e., independent variables).\n",
    "\n",
    "While <code>KNN('uniform')</code> produced good performance metrics, its scores are still comparatively lower than in <code>KNN('distance')</code>. Results have shown that <code>KNN('uniform')</code> produced one false positive and one false negative predictions, thus, having misclassifications to its results. Because of this, the <code>F1-score</code> for both cancer and no cancer class are 88%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "- The <code>weights = 'distance'</code> generated larger $k$ than in <code>weights = 'uniform'</code>. This implies that uniform-based weighting strategy provides higher variance and lower bias than <code>weights = 'distance'</code>. Hence, we may expect that <code>weights = 'distance'</code> would tend to overfit since it can overly prioritize the closest neighbor and disregard the other nearest neighbors if they are bit a further away.    \n",
    "\n",
    "\n",
    "\n",
    "It is expected that using <code>weights = 'distance'</code> would tend to overfit more indeed. The reason for this is that it can potentially overly prioritize the closest neighbor and disregard the other nearest neighbors if they are a bit further away. \n",
    "\n",
    "The <code>weights = 'uniform'</code> ensures that even if some of the nearest neighbors are a bit further away, they still count as much towards the prediction.\n",
    "\n",
    "This is a good illustration of the bias-variance tradeoff.\n",
    "- The <code>distance</code> reduces the bias by down-weighting data points that are less similar, but by doing that it increases the variance since the prediction relies more on individual data points of the training sample.\n",
    "- The <code>uniform</code> does the opposite, it reduces variance by ensuring each of the nearest neighbors has the same contribution, thus reducing the dependence in individual training data points, but at the cost of equally considering nearest neighbors which can end up being still quite distant from the observation to label, which leads to larger bias in return.\n",
    "\n",
    "To conclude, you might want to go for distance when you feel like your model is underfitting, which could be characterized by many \"average\" predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6a2dc670f3436433c0efae6fb324965c1072d8aef0b90287abce79ee9328779"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
