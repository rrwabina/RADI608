{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RADI608: Data Mining and Machine Learning**\n",
    "\n",
    "### Assignment: K-Nearest Neighbors \n",
    "**Romen Samuel Rodis Wabina** <br>\n",
    "Student, PhD Data Science in Healthcare and Clinical Informatics <br>\n",
    "Clinical Epidemiology and Biostatistics, Faculty of Medicine (Ramathibodi Hospital) <br>\n",
    "Mahidol University"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, make_scorer, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "#### Perform K-Nearest Neighbors to predict patient have a cancer using <code>weights = 'distance'</code>.\n",
    "\n",
    "**SOLUTION** <br>\n",
    "The class distribution is imbalanced with 22 patients having cancer while 40 patients having no cancer. Hence, it is important to perform resampling techniques (i.e., undersampling, oversampling, SMOTE). Here, we performed oversampling using Synthetic Minority Oversampling Technique (SMOTE) as presented below as <code>smote = SMOTE()</code>. We also verified the dataset in terms of their data types (i.e., <code>float</code> for $\\mathbf{X}$ while <code>int</code> for $\\mathbf{y}$) to ensure proper data modeling in KNN. No missing values were detected in the dataset. In addition, we normalize the given dataset through standardization by removing the mean and scaling to unit variance. We also split the data into 80:20 ratio between training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_colon = pd.read_csv('../data/colon.csv')\n",
    "XX = df_colon.drop('Class', axis = 1)\n",
    "yy = df_colon['Class']\n",
    "\n",
    "X, y = XX.to_numpy(), yy.to_numpy()\n",
    "y = y.flatten()\n",
    "\n",
    "smote = SMOTE()\n",
    "X, y = smote.fit_resample(X, y)\n",
    "\n",
    "random.seed(413)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used <code>weights = 'distance'</code> for this KNN model, that initializes the weight assigned to points in the neighbourhood. Since we used <code>distance</code>, the closer neighbours of a query point will have a greater influence than neighbours which are further away. In the code below, we initialized the <code>n_neighbors</code> as a range between $[2, 12)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(X_train, X_test, y_train, y_test, \n",
    "         model, param_grid = {'n_neighbors': np.arange(1, 13, 1)}):\n",
    "\n",
    "    random.seed(413)\n",
    "    cv = StratifiedShuffleSplit(n_splits = 10, random_state = 42)\n",
    "    grid = GridSearchCV(model, param_grid = param_grid, cv = cv, refit = 'f1_micro')\n",
    "    grid.fit(X_train, y_train) \n",
    "\n",
    "    print(f\"The best parameters are {grid.best_params_} with\" + f\" a score of {grid.best_score_:.2f}\")\n",
    "    \n",
    "    yhat = grid.predict(X_test)\n",
    "\n",
    "    print('======================= Confusion Matrix =======================')\n",
    "    print(confusion_matrix(y_test, yhat))\n",
    "    \n",
    "    print('==================== Classification Report =====================')\n",
    "    print(classification_report(y_test, yhat, target_names = ['No Cancer', 'Cancer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'n_neighbors': 4} with a score of 0.90\n",
      "======================= Confusion Matrix =======================\n",
      "[[8 0]\n",
      " [0 8]]\n",
      "==================== Classification Report =====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   No Cancer       1.00      1.00      1.00         8\n",
      "      Cancer       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           1.00        16\n",
      "   macro avg       1.00      1.00      1.00        16\n",
      "weighted avg       1.00      1.00      1.00        16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_distance = KNeighborsClassifier(weights = 'distance')\n",
    "main(X_train, X_test, y_train, y_test, model_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>SVM(weights = 'distance')</code> produced perfect performance metrics. Results have shown that there are no misclassifications among the two classes in the given dataset - that is, no false positive and false negative predictions. Because of this, the <code>F1-score</code> produced 100% accuracy for two classes. It is important that we will use F1-score as the primary performance metrics since we utilized a high-dimensional dataset (i.e., number of features is larger than samples). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "#### Perform KNN to predict patient have a cancer using <code>weights = 'uniform'</code>\n",
    "\n",
    "We used <code>weights = 'uniform'</code> for this KNN model, that initializes the weight assigned to points in the neighbourhood. Since we used <code>uniform</code>, this means that all neighborsget an equally weighted *vote* about an observation's class. In the code below, we initialized the <code>n_neighbors</code> as a range between $[2, 12)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'n_neighbors': 12} with a score of 0.87\n",
      "======================= Confusion Matrix =======================\n",
      "[[6 2]\n",
      " [1 7]]\n",
      "==================== Classification Report =====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   No Cancer       0.86      0.75      0.80         8\n",
      "      Cancer       0.78      0.88      0.82         8\n",
      "\n",
      "    accuracy                           0.81        16\n",
      "   macro avg       0.82      0.81      0.81        16\n",
      "weighted avg       0.82      0.81      0.81        16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_distance = KNeighborsClassifier(weights = 'uniform')\n",
    "main(X_train, X_test, y_train, y_test, model_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "It is expected that using <code>weights = 'distance'</code> would tend to overfit more indeed. The reason for this is that it can potentially overly prioritize the closest neighbor and disregard the other nearest neighbors if they are a bit further away. \n",
    "\n",
    "The <code>weights = 'uniform'</code> ensures that even if some of the nearest neighbors are a bit further away, they still count as much towards the prediction.\n",
    "\n",
    "This is a good illustration of the bias-variance tradeoff.\n",
    "- The <code>distance</code> reduces the bias by down-weighting data points that are less similar, but by doing that it increases the variance since the prediction relies more on individual data points of the training sample.\n",
    "- The <code>uniform</code> does the opposite, it reduces variance by ensuring each of the nearest neighbors has the same contribution, thus reducing the dependence in individual training data points, but at the cost of equally considering nearest neighbors which can end up being still quite distant from the observation to label, which leads to larger bias in return.\n",
    "\n",
    "To conclude, you might want to go for distance when you feel like your model is underfitting, which could be characterized by many \"average\" predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6a2dc670f3436433c0efae6fb324965c1072d8aef0b90287abce79ee9328779"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
